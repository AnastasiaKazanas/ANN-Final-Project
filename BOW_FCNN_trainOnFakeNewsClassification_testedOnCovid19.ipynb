{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnastasiaKazanas/ANN-Final-Project/blob/main/BOW_FCNN_trainOnFakeNewsClassification_testedOnCovid19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "bFKXKDfArPNO"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uiAvJ6yEr4dR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382df77b-6183-45a6-c122-03d900468a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/saurabhshahane/fake-news-classification?dataset_version_number=77...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92.1M/92.1M [00:01<00:00, 83.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset preview:\n",
            "   Unnamed: 0                                              title  \\\n",
            "0           0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...   \n",
            "1           1                                                NaN   \n",
            "2           2  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...   \n",
            "3           3  Bobby Jindal, raised Hindu, uses story of Chri...   \n",
            "4           4  SATAN 2: Russia unvelis an image of its terrif...   \n",
            "\n",
            "                                                text  label  \n",
            "0  No comment is expected from Barack Obama Membe...      1  \n",
            "1     Did they post their votes for Hillary already?      1  \n",
            "2   Now, most of the demonstrators gathered last ...      1  \n",
            "3  A dozen politically active pastors came here f...      0  \n",
            "4  The RS-28 Sarmat missile, dubbed Satan 2, will...      1  \n",
            "\n",
            "Dataset statistics:\n",
            "         Unnamed: 0         label\n",
            "count  72134.000000  72134.000000\n",
            "mean   36066.500000      0.514404\n",
            "std    20823.436496      0.499796\n",
            "min        0.000000      0.000000\n",
            "25%    18033.250000      0.000000\n",
            "50%    36066.500000      1.000000\n",
            "75%    54099.750000      1.000000\n",
            "max    72133.000000      1.000000\n",
            "\n",
            "Missing values per column:\n",
            "Unnamed: 0      0\n",
            "title         558\n",
            "text           39\n",
            "label           0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Kaggle dataset download\n",
        "path = kagglehub.dataset_download(\"saurabhshahane/fake-news-classification\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/root/.cache/kagglehub/datasets/saurabhshahane/fake-news-classification/versions/77/WELFake_Dataset.csv\")\n",
        "\n",
        "print(\"Dataset preview:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(missing_values)\n",
        "df.dropna(subset=['text', 'label'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Add filtering\n",
        "new_data = []\n",
        "new_labels = []\n",
        "for text, label in zip(df['processed_text'], df['label']):\n",
        "    if len(text) < 50 and text not in new_data:\n",
        "        new_data.append(text)\n",
        "        new_labels.append(label)\n",
        "\n",
        "filtered_df = pd.DataFrame({'text': new_data, 'label': new_labels})\n",
        "\n",
        "# Split the dataset\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    filtered_df['text'], filtered_df['label'], test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "eRQdv5sq2kZt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z8msfX1EtEXY"
      },
      "outputs": [],
      "source": [
        "# Tfidf Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train = vectorizer.fit_transform(train_texts).toarray()\n",
        "X_test = vectorizer.transform(test_texts).toarray()\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = torch.tensor(train_labels.values, dtype=torch.long)\n",
        "test_labels = torch.tensor(test_labels.values, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), train_labels)\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define NN\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 128\n",
        "output_dim = 2\n",
        "\n",
        "model = TextClassifier(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        outputs = model(X_batch)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        y_pred.extend(preds.numpy())\n",
        "        y_true.extend(y_batch.numpy())\n"
      ],
      "metadata": {
        "id": "TH8Ve8fUMkgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13793ca6-89fe-45f5-df49-2c0f62ce9811"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 5.4985\n",
            "Epoch 2/10, Loss: 5.0999\n",
            "Epoch 3/10, Loss: 4.6293\n",
            "Epoch 4/10, Loss: 4.0359\n",
            "Epoch 5/10, Loss: 3.3106\n",
            "Epoch 6/10, Loss: 2.6179\n",
            "Epoch 7/10, Loss: 1.9887\n",
            "Epoch 8/10, Loss: 1.4281\n",
            "Epoch 9/10, Loss: 1.0458\n",
            "Epoch 10/10, Loss: 0.8120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = kagglehub.dataset_download(\"invalizare/covid-19-fake-news-dataset\")\n",
        "dataset_dir = \"/root/.cache/kagglehub/datasets/invalizare/covid-19-fake-news-dataset/\"\n",
        "\n",
        "file_name = None\n",
        "for root, dirs, files in os.walk(dataset_dir):\n",
        "    for file in files:\n",
        "        if \"Val.csv\" in file:\n",
        "            file_name = os.path.join(root, file)\n",
        "            break\n",
        "\n",
        "# Load the dataset\n",
        "covid_df = pd.read_csv(file_name)\n",
        "\n",
        "# Preprocess the text\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text.strip()\n",
        "\n",
        "covid_df['processed_text'] = covid_df['tweet'].apply(preprocess_text)\n",
        "covid_df.dropna(subset=['processed_text', 'label'], inplace=True)\n",
        "\n",
        "covid_df['label'] = pd.to_numeric(covid_df['label'], errors='coerce')\n",
        "covid_df.dropna(subset=['label'], inplace=True)\n",
        "covid_df['label'] = covid_df['label'].astype(int)\n",
        "\n",
        "if covid_df.empty:\n",
        "    print(\"No valid rows in the COVID-19 dataset after preprocessing. Skipping evaluation.\")\n",
        "else:\n",
        "    # Process data if there are valid rows\n",
        "    X_covid = vectorizer.transform(covid_df['processed_text']).toarray()\n",
        "    covid_labels = torch.tensor(covid_df['label'].values, dtype=torch.long)\n",
        "\n",
        "    covid_dataset = TensorDataset(torch.tensor(X_covid, dtype=torch.float32), covid_labels)\n",
        "    covid_loader = DataLoader(covid_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in covid_loader:\n",
        "            outputs = model(X_batch)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            y_pred.extend(preds.numpy())\n",
        "            y_true.extend(y_batch.numpy())\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f\"\\nModel Accuracy on COVID-19 Dataset: {accuracy:.2f}\")\n",
        "\n",
        "    print(\"\\nClassification Report on COVID-19 Dataset:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['False', 'True']))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(conf_matrix, display_labels=['False', 'True'])\n",
        "    disp.plot(cmap='Blues')\n",
        "    plt.title('Confusion Matrix - COVID-19 Dataset')\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ_2Ct00LAVf",
        "outputId": "f4d41642-3606-47f3-f510-752f4c3958dc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No valid rows in the COVID-19 dataset after preprocessing. Skipping evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F1DdVKJX1LIh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}