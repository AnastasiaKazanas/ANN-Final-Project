{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "id": "bFKXKDfArPNO"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import random\n",
        "import torch\n",
        "import pickle\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# !pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiAvJ6yEr4dR",
        "outputId": "ba065c2c-4716-4ba4-bfca-0f5661448429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values per column:\n",
            "Unnamed: 0                 0\n",
            "author                     0\n",
            "statement                  0\n",
            "target                     0\n",
            "BinaryNumTarget            0\n",
            "manual_keywords            0\n",
            "tweet                      0\n",
            "5_label_majority_answer    0\n",
            "3_label_majority_answer    0\n",
            "dtype: int64\n",
            "   Unnamed: 0      author                                          statement  \\\n",
            "0           0  D.L. Davis  End of eviction moratorium means millions of A...   \n",
            "1           1  D.L. Davis  End of eviction moratorium means millions of A...   \n",
            "2           2  D.L. Davis  End of eviction moratorium means millions of A...   \n",
            "3           3  D.L. Davis  End of eviction moratorium means millions of A...   \n",
            "4           4  D.L. Davis  End of eviction moratorium means millions of A...   \n",
            "\n",
            "   target  label                 manual_keywords  \\\n",
            "0    True    1.0  Americans, eviction moratorium   \n",
            "1    True    1.0  Americans, eviction moratorium   \n",
            "2    True    1.0  Americans, eviction moratorium   \n",
            "3    True    1.0  Americans, eviction moratorium   \n",
            "4    True    1.0  Americans, eviction moratorium   \n",
            "\n",
            "                                                text 5_label_majority_answer  \\\n",
            "0  @POTUS Biden Blunders - 6 Month Update\\n\\nInfl...            Mostly Agree   \n",
            "1  @S0SickRick @Stairmaster_ @6d6f636869 Not as m...             NO MAJORITY   \n",
            "2  THE SUPREME COURT is siding with super rich pr...                   Agree   \n",
            "3  @POTUS Biden Blunders\\n\\nBroken campaign promi...            Mostly Agree   \n",
            "4  @OhComfy I agree. The confluence of events rig...                   Agree   \n",
            "\n",
            "  3_label_majority_answer  \n",
            "0                   Agree  \n",
            "1                   Agree  \n",
            "2                   Agree  \n",
            "3                   Agree  \n",
            "4                   Agree  \n"
          ]
        }
      ],
      "source": [
        "#load data from kaggle\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import kagglehub\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "# Download latest version\n",
        "path_Fake_News_Classification = kagglehub.dataset_download(\"saurabhshahane/fake-news-classification\")\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"sudishbasnet/truthseekertwitterdataset2023\")\n",
        "dataset_dir = \"/root/.cache/kagglehub/datasets/sudishbasnet/truthseekertwitterdataset2023/\"\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df_Fake_News_Classification = pd.read_csv(\"/root/.cache/kagglehub/datasets/saurabhshahane/fake-news-classification/versions/77/WELFake_Dataset.csv\")\n",
        "\n",
        "# Dynamically locate the CSV file\n",
        "file_name = None\n",
        "for root, dirs, files in os.walk(dataset_dir):\n",
        "    for file in files:\n",
        "        if \"Truth_Seeker_Model_Dataset\" in file and file.endswith(\".csv\"):\n",
        "            file_name = os.path.join(root, file)\n",
        "            break\n",
        "\n",
        "df = pd.read_csv(file_name)\n",
        "\n",
        "\n",
        "# Handle missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(missing_values)\n",
        "df.dropna(subset=['tweet', 'BinaryNumTarget'], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "df.rename(columns={'tweet': 'text', 'BinaryNumTarget': 'label'}, inplace=True)\n",
        "# Check the first few rows of the dataset\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IrA6-jVsJ5I",
        "outputId": "86cd6320-b56f-40ac-b8b5-d7baa02febf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per column:\n",
            " Unnamed: 0                 0\n",
            "author                     0\n",
            "statement                  0\n",
            "target                     0\n",
            "label                      0\n",
            "manual_keywords            0\n",
            "text                       0\n",
            "5_label_majority_answer    0\n",
            "3_label_majority_answer    0\n",
            "dtype: int64\n",
            "\n",
            "Number of rows with missing values: 0\n",
            "\n",
            "Rows with missing values:\n",
            "Empty DataFrame\n",
            "Columns: [Unnamed: 0, author, statement, target, label, manual_keywords, text, 5_label_majority_answer, 3_label_majority_answer]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "#check what's missing in the data\n",
        "\n",
        "# Check for missing values in the dataset\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Check how many rows have missing data\n",
        "rows_with_missing_values = df[df.isnull().any(axis=1)]\n",
        "\n",
        "# Print out the missing values summary and rows with missing data\n",
        "print(\"Missing values per column:\\n\", missing_values)\n",
        "print(\"\\nNumber of rows with missing values:\", rows_with_missing_values.shape[0])\n",
        "\n",
        "# Preview the rows with missing values\n",
        "print(\"\\nRows with missing values:\")\n",
        "print(rows_with_missing_values.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QypO8jkCsXf9",
        "outputId": "d7ee6c68-493e-4c0c-94bd-848cff81ce68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@POTUS Biden Blunders - 6 Month Update\\n\\nInflation, Delta mismanagement, COVID for kids, Abandoning Americans in Afghanistan, Arming the Taliban, S. Border crisis, Breaking job growth, Abuse of power (Many Exec Orders, $3.5T through Reconciliation, Eviction Moratorium)...what did I miss?']\n",
            "287\n",
            "                                                text  label\n",
            "0  @POTUS Biden Blunders - 6 Month Update\\n\\nInfl...    1.0\n",
            "1  @S0SickRick @Stairmaster_ @6d6f636869 Not as m...    1.0\n",
            "2  THE SUPREME COURT is siding with super rich pr...    1.0\n",
            "3  @POTUS Biden Blunders\\n\\nBroken campaign promi...    1.0\n",
            "4  @OhComfy I agree. The confluence of events rig...    1.0\n",
            "\n",
            " ['@fattycattie @robquinnpc @SammyTMcCarty @WCCO The rifle was perfectly legal for him to possess and was never transported across state lines. Kyle works daily in Kenosha, lives closer to Kenosha than any of his assailants, and did not employ deadly force until it became his sole option to avoid grievous harm. JR was a pedo.', 'never forget, Matt Santos ran on Medicare for All in 2005, and won the Presidency. thats my President.'] \n",
            " [0.0, 1.0]\n",
            "133211\n",
            "133211\n"
          ]
        }
      ],
      "source": [
        "#clean the data of missing values\n",
        "\n",
        "# Ensure that all values in 'text' are strings\n",
        "df['text'] = df['text'].fillna('')  # Fill NaN values with an empty string\n",
        "texts = df['text'].astype(str).tolist()  # Convert to list of strings\n",
        "\n",
        "# # map labels to binary values\n",
        "# df['label'] = df['label'].map({'true': 1, 'false': 0})\n",
        "\n",
        "# Check the first few entries\n",
        "print(texts[:1])\n",
        "\n",
        "print(len(texts[0]))\n",
        "\n",
        "# Split the dataset\n",
        "# train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "#     df['text'], df['label'], test_size=0.2, random_state=42\n",
        "\n",
        "dataset = df[['text', 'label']]\n",
        "print(dataset.head())\n",
        "dataset = shuffle(dataset, random_state=42).reset_index(drop=True)\n",
        "texts = dataset['text'].tolist()\n",
        "labels = dataset['label'].tolist()\n",
        "print('\\n', texts[:2], '\\n', labels[:2])\n",
        "\n",
        "# count, sec, thr, you = 0, 0, 0, 0\n",
        "# new_dic = []\n",
        "# for i, k in enumerate(texts):\n",
        "#   if len(k) < 200:\n",
        "#     thr += 1\n",
        "#     # if len(k) > 100:\n",
        "#     #   print(k, labels[i])\n",
        "#   if k == ' ':\n",
        "#     count += 1\n",
        "\n",
        "#   if k[:4] == 'http':\n",
        "#     you += 1\n",
        "\n",
        "#   if k not in new_dic:\n",
        "#     new_dic.append(k)\n",
        "\n",
        "filtered_texts = []\n",
        "filtered_labels = []\n",
        "for t, l in zip(texts, labels):\n",
        "  if len(t) > 50 and t not in filtered_texts:\n",
        "    filtered_texts.append(t)\n",
        "    filtered_labels.append(l)\n",
        "\n",
        "print(len(filtered_texts))\n",
        "print(len(filtered_labels))\n",
        "# print(count, sec, thr, you, len(texts), len(new_dic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8msfX1EtEXY",
        "outputId": "b17a673c-003b-4491-ee7d-d15eb7f052f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1030, 19101,  ...,     0,     0,     0],\n",
            "        [  101,  2196,  5293,  ...,     0,     0,     0],\n",
            "        [  101,  1030,  3021,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1030, 29044,  ...,     0,     0,     0],\n",
            "        [  101,  1030,  7775,  ...,     0,     0,     0],\n",
            "        [  101,  1030,  6798,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
            "133211\n"
          ]
        }
      ],
      "source": [
        "#tokenize with bert\n",
        "#mattias says to put this into batches (XXX - ?)\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to tokenize the text, adjust the max_length parameter based on data analysis (XXX)\n",
        "def encode_text(texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
        "\n",
        "# Tokenize the text data\n",
        "#train_encodings = encode_text(texts[:10])\n",
        "texts_encodings = tokenizer(list(filtered_texts), padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "# Convert labels to tensors\n",
        "filtered_labels = torch.tensor(list(filtered_labels))\n",
        "\n",
        "# Check the tokenized data\n",
        "print(texts_encodings)\n",
        "print(len(filtered_texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYZn4yebLiDL"
      },
      "outputs": [],
      "source": [
        "# with open(\"temp.pkl\", \"rb\") as fOut:\n",
        "#   lol = pickle.load(fOut)\n",
        "# # print(lol['sentences'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdn8rrRdEQeV",
        "outputId": "cb2ee87e-52d6-4b45-b5de-b5504461310c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72134\n"
          ]
        }
      ],
      "source": [
        "# print(len(lol['sentences']))\n",
        "# # print(len(lol['sentences'].index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdoQKFz_FLVD",
        "outputId": "26a9deda-a9a7-41cd-fd21-8d52589df6d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unnamed: 0                                                60264\n",
            "title         Elon Muskâ€™s Tesla Stock Up $2 Billion Since Jo...\n",
            "text          Although Tesla CEO Elon Musk shocked Silicon V...\n",
            "label                                                         0\n",
            "Name: 60264, dtype: object\n",
            "tensor(0)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yt2bqJyx1XoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "input_ids = texts_encodings['input_ids'].numpy()\n",
        "attention_mask = texts_encodings['attention_mask'].numpy()\n",
        "token_type_ids = texts_encodings['token_type_ids'].numpy()\n",
        "labels = np.array(labels)\n",
        "\n",
        "with h5py.File('Twitter.h5', 'w') as hf:\n",
        "    hf.create_dataset('input_ids', data=input_ids, compression=\"gzip\")\n",
        "    hf.create_dataset('attention_mask', data=attention_mask, compression=\"gzip\")\n",
        "    hf.create_dataset('token_type_ids', data=token_type_ids, compression=\"gzip\")\n",
        "    hf.create_dataset('labels', data=filtered_labels, compression=\"gzip\")\n",
        "    dt = h5py.string_dtype(encoding='utf-8')\n",
        "    hf.create_dataset('texts', data=filtered_texts, dtype=dt)\n"
      ],
      "metadata": {
        "id": "Ohh2nfsjtASp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Open the HDF5 file in read mode\n",
        "with h5py.File('Dataset_first.h5', 'r') as hf:\n",
        "    # Load the datasets\n",
        "    input_ids = np.array(hf['input_ids'])\n",
        "    attention_mask = np.array(hf['attention_mask'])\n",
        "    labels = np.array(hf['labels'])\n",
        "\n",
        "# Check the loaded data\n",
        "print(\"Input IDs shape:\", input_ids.shape)\n",
        "print(\"Attention Mask shape:\", attention_mask.shape)\n",
        "print(\"Labels shape:\", labels.shape)\n",
        "\n",
        "# Example: Access a single data point\n",
        "print(\"First input_ids:\", input_ids[10])\n",
        "print(\"First attention_mask:\", attention_mask[10])\n",
        "print(\"First label:\", labels[10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_ot0J8y5Bi1",
        "outputId": "68c0862e-4000-4932-c7ab-db0e9d57d3da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs shape: (72134, 512)\n",
            "Attention Mask shape: (72134, 512)\n",
            "Labels shape: (72134,)\n",
            "First input_ids: [  101  4419  2739  1996  2047  2259  2335  2988  3041  2023  2733  2008\n",
            "  6521  2371 26608  2044  8398  5496  2032  1997  4487 14540 18232 24228\n",
            "  2138  2002 28667 13901  2370  2013  1996  3425  2533  1055  4812  2046\n",
            "  1996  8398  3049  1055  7208  2000  3607  1012  8398 22416  1037  5164\n",
            "  1997 23862  2012  6521  1010  2040  2052  2101  2175  2006  2000  2360\n",
            "  8398  1055 16360 20026  5685  2075  2001  1996  2087 28284  2724  2002\n",
            "  5281  2004  1037  2270  7947  1010  2429  2000  1996  2335  1012 16360\n",
            "  1012 21510  2638  5380  1010  1040  1011 10250 10128  1012  1010  1056\n",
            " 28394  3064  5958  2008  4905  2236  5076  6521  2085  4282  2129  3060\n",
            "  1011  4841  2514  2044  2002  2001  7283 26608  2011  2343  8398  2058\n",
            "  2010 28667 10383  2140  1999  1996  3607  4812  1012  2006  1037 16110\n",
            "  1999  2089  1010  5380  3615  2000  6521  2004  2200  4795  1010  2077\n",
            "  5815  1010  1045  2228  2002  1055  1037 16939  1010  1998  1045  2228\n",
            "  2008  2002  7078  7164  2008  2009  1055  2010  3105  2000  2562 14302\n",
            "  1999  2037  2173  1012  2000  5076  6521  1010  2129  2515  2009  2514\n",
            "  2000  2022  7944  1004 26608  1029  2085  2017  2113  2129  1996  3060\n",
            "  4841  2017  4487 21338  2229  5051 10985  2514  1010  2016  1056 28394\n",
            "  3064  1012  2000  5076  6521  1010  2129  2515  2009  2514  2000  2022\n",
            "  7944  1004 26608  1029  2085  2017  2113  2129  1996  3060  4841  2017\n",
            "  4487 21338  2229  5051 10985  2514 21510  2638  5380  1006  1030 16360\n",
            " 17848  3170  5880  2015  1007  2244  2321  1010  2418  3406  2029  1996\n",
            "  2280  6458  1010  2585  8359  8235  2135  5838  1024  2009  1055  5793\n",
            "  2008 21510  2638  5380 16424  2317  2111  1012  2008  2052  2191  2014\n",
            "  1037  2304 10514 28139 22911  2923  1012  2009  1005  1055  5793  2008\n",
            " 21510  2638  5380 16424  2317  2111  1012  2008  2052  2191  2014  1037\n",
            "  2304 10514 28139 22911  2923  1012 27263  1012 10474  1012  4012  1013\n",
            " 10751  2099  4160  2290  2581 20348 14194  2585  1037  1012  8359  1010\n",
            "  3781  1012  1006  1030  6458 20464 17007  2063  1007  2244  2385  1010\n",
            "  2418  6673  2863  2064  2022  9200  1010  2021  1999 21510  2638  5380\n",
            "  2553  1010  2009  1055  1037  6841   102     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "First attention_mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "First label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNYOZgON_G4y",
        "outputId": "fb9bfafb-25d6-4c49-ce60-034529ff8d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hash comparison passed. Data is identical.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}